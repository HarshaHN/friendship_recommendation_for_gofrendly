# -*- coding: utf-8 -*-
"""two.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bZbq0ZY1C3gIyePJB9F0N6eGW93PYBG3
"""

#%%---------------------------------
import os
os.chdir('/content/drive/My Drive/gofrendly')
#os.getcwd()

!pip install dgl
import torch

device = torch.device('cuda'); #print(device) #cuda
#print(torch.cuda.current_device()) #0
#print(torch.cuda.device(0)) #<torch.cuda.device at 0x7fc1f955b198>
#print(torch.cuda.device_count()) #1
print(torch.cuda.get_device_name(0)) #'Tesla P100'
#print(torch.cuda.is_available()) #True
#print(os.cpu_count()) #2
#!nvidia-smi

"""
VERSION = "nightly" 
#param ["20200220","nightly", "xrt==1.15.0"]
!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
!python pytorch-xla-env-setup.py --version $VERSION

# imports the torch_xla package
import torch_xla
import torch_xla.core.xla_model as xm

device = xm.xla_device() """

#%%-------------------------------------------------
# Load the files
import pickle
import dgl
 
choice = 1
 
if choice == 1:
  # 15,000 | 48 
  with open('data/colab.pkl', 'rb') as f: [_, X, _, _] = pickle.load(f) #[G, X, trainpos, trainneg]
  with open('data/nw.pkl', 'rb') as f: [pos, neg] = pickle.load(f)
  with open('data/valpos.pkl', 'rb') as f: valpos = pickle.load(f)
elif choice == 2:
  # 15,000 | 816
  with open('data/X.pkl', 'rb') as f: X = pickle.load(f)
  with open('data/nw.pkl', 'rb') as f: [pos, neg] = pickle.load(f)
  with open('data/valpos.pkl', 'rb') as f: valpos = pickle.load(f)
elif choice == 3:
  # 120,000 | 48
  with open('data/all_X.pkl', 'rb') as f: X = pickle.load(f)
  with open('data/all_nw.pkl', 'rb') as f: [pos, neg] = pickle.load(f)
  with open('data/all_valpos.pkl', 'rb') as f: valpos = pickle.load(f)
  with open('data/nw.pkl', 'rb') as f: [pos2, neg2] = pickle.load(f)
  with open('data/valpos.pkl', 'rb') as f: valpos2 = pickle.load(f)
  #with open('data/idx.pkl', 'rb') as f: idx = pickle.load(f)
 
#with open('one.pkl', 'rb') as f: [emb] = pickle.load(f)
 
"""Move all the tensors to GPU """
import torch
X = X.to(device); print(X.is_cuda); print(X.shape)
#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'); print(device)
 
# pos, neg, valpos = torch.tensor(pos), torch.tensor(neg), torch.tensor(valpos)
# pos, neg, valpos = pos.to(device), neg.to(device), valpos.to(device)
# print(pos.is_cuda, neg.is_cuda, valpos.is_cuda)

#%%---------------------------------
import dgl
import time

t0 = time.time()
def makedgl(num, pos):
    G = dgl.DGLGraph()
    G.add_nodes(num)
    G.add_edges(G.nodes(), G.nodes()) #self loop all
    G.add_edges(*zip(*pos)) #add edges list(zip(*pos))
    G = dgl.to_bidirected(G) 
    G = dgl.graph(G.edges(), 'user', 'frd')
    print('-> Graph G has %d nodes' % G.number_of_nodes(), 'with %d edges' % (G.number_of_edges()/2)) 
    return G

G = makedgl(num=len(X), pos=pos)

# Random walk
import dgl
import torch
rw = dgl.sampling.RandomWalkNeighborSampler(G = G, 
                                            random_walk_length = 64,
                                            random_walk_restart_prob=0,
                                            num_random_walks = 32,
                                            num_neighbors = 16,
                                            weight_column = 'w')

rwG = rw(torch.LongTensor(G.nodes()))
rwG.edata['w'] = rwG.edata['w'].float(); 

del rw, rwG.edata['_ID'], G 
print('-> Graph G has %d nodes' % rwG.number_of_nodes(), 'with %d edges' % (rwG.number_of_edges())) # rwG.is_readonly
print('Time taken:', time.time() - t0)

# ng.predecessors(1) # ng.edata['w'][ng.edge_ids(*ng.in_edges(1))]
#with open('data/rwG_3.pkl', 'wb') as f: pickle.dump(rwG, f)

#%%----------------------
""" 01. Graph Neural Network """

import nn
import pipe
import torch
import time
import importlib; importlib.reload(nn); importlib.reload(pipe)

# x = X
# x = X[:, :3]
# x = X[:, 3:48]
# x = X[:, 48:]
# with open('data/one.pkl', 'rb') as f: x = pickle.load(f)
# neg = neg; pos = pos  #402761: (2134,400627) #72382: (16063,56319)
# with open('data/rwG_3.pkl', 'rb') as f: rwG = pickle.load(f) # 121109: 180963, 404803
 
fdim = X.shape[1]
indices = list(range(X.shape[0]))
#indices = idx
 
twomodel = nn.gnet( graph = rwG,
                    nodeemb = X, #.to(device),
                    convlayers = [[fdim, fdim]],#, [48,45,45], [45,42,42], [42,39,39], [39,36,36]],
                    #convlayers = [[fdim, fdim], [fdim, fdim, fdim], [fdim, fdim, fdim]],
                    output_size = fdim,
                    dropout = 0.00,
                    lr = 3e-4,
                    opt = 'RMSprop', # Rprop, RMSprop, Adamax, AdamW, Adagrad, Adadelta, SGD, Adam
                    select_loss = 'cosine', #'pinsage'
                    loss_margin = 0.25, # 0.25
                    pos = pos, #16063
                    neg = neg, #2134
                    val_pos = valpos,
                    idx = indices)
 
twomodel.to(device)
 
# nn01_loss, nn01_hr, nn01_mrr = 0.069, 27.1, 0.8 
print(X.shape)

#%%---------------------------------
""" 03. a. Training """
 
epochs = 300
lr = 3e-4 #3e-4 #6e-4
loss_interval, eval_interval, emb_interval = 5, 50, 50
intervals = [loss_interval, eval_interval, emb_interval]
 
twomodel.optimizer  = getattr(torch.optim, 'RMSprop')(twomodel.net.parameters(), lr)
[newemb, train_eval, val_eval, loss_values, embs] = twomodel.train(epochs, intervals)

#%%---------------------------------
import pickle
#with open('data/two_.pkl', 'wb') as f: pickle.dump([newemb, train_eval, val_eval, loss_values, embs], f)
print(max(train_eval), max(val_eval))

#%%---------------------------------
import pickle
# with open('data/two_training.pkl', 'wb') as f: pickle.dump([newemb, train_eval, val_eval, loss_values, embs], f)
# with open('data/two_s1.pkl', 'wb') as f: pickle.dump([newemb, train_eval, val_eval, loss_values, embs], f)
# with open('data/two_s2.pkl', 'wb') as f: pickle.dump([newemb, train_eval, val_eval, loss_values, embs], f)

with open('data/two_training.pkl', 'rb') as f: [newemb, train_eval, val_eval, loss_values, embs] = pickle.load(f) #400, 50, 100 #3e-4
# with open('data/two_s1.pkl', 'rb') as f: [newemb, train_eval, val_eval, loss_values, embs] = pickle.load(f) #
# with open('data/two_s2.pkl', 'rb') as f: [newemb, train_eval, val_eval, loss_values, embs] = pickle.load(f)

#with open('data/two.pkl', 'wb') as f: pickle.dump(embs[-1], f)

nn01_loss, nn01_hr, nn01_mrr = 0.069, 27.1, 0.8 
nn02_loss, nn02_hr, nn02_mrr = 0.073, 34.1, 1.0

max(val_eval)

#%%---------------------------------
import matplotlib.pyplot as plt 
import seaborn as sns

epochs = 400
#lr = 3e-4 #6e-4
loss_interval, eval_interval, emb_interval = 5, 50, 100
""
# Plot the loss values
fig1 = plt.figure(1); plt.grid()
plt.plot(range(1,epochs+1), loss_values)
plt.xticks([1] + list(range(100, epochs+1, 100)))
fig1.suptitle('Loss value Vs. Epoch'); plt.xlabel('Epoch'); plt.ylabel('Loss value')
plt.axhline(y=nn01_loss, color='green', linestyle='--')
plt.text(x=0, y=nn01_loss+0.02, s='NN01 ='+str(nn01_loss), fontsize=12, color='green')
plt.show()

#fig1.savefig('diagrams/plots/nn01_lossvalues.jpg')
#fig1.savefig('diagrams/plots/nn01_s1_lossvalues.jpg')
#fig1.savefig('diagrams/plots/nn01_s2_lossvalues.jpg')
""

# Plot the hit-rate and MRR
train_hr = [i for i,j in train_eval]; train_mrr = [j for i,j in train_eval] 
val_hr = [i for i,j in val_eval]; val_mrr = [j for i,j in val_eval]  

fig2 = plt.figure(2); plt.grid()
plt.xticks( list(range(len(train_hr))), [eval_interval] + list(range(eval_interval*2, eval_interval * (1+len(train_hr)), eval_interval)))
plt.plot(train_hr, label = 'train'); plt.plot(val_hr, label = 'valid'); plt.legend(loc='upper left')
fig2.suptitle('Hit-rate Vs. Epoch'); plt.xlabel('Epoch'); plt.ylabel('Hitrate')
# plt.plot([nn01_hr]*len(val_hr), label = 'NN02');
plt.axhline(y=nn01_hr, color='green', linestyle='--')
plt.text(x=0, y=nn01_hr+0.5, s='NN01 =' + str(nn01_hr), fontsize=12, color='green')

#fig2.savefig('diagrams/plots/nn01_hr.jpg')
#fig2.savefig('diagrams/plots/nn01_s1_hr.jpg')
#fig2.savefig('diagrams/plots/nn01_s2_hr.jpg')

""
fig3 = plt.figure(3); plt.grid()
plt.xticks( list(range(len(train_mrr))), [eval_interval] + list(range(eval_interval*2, eval_interval * (1+len(train_mrr)), eval_interval)))
plt.plot(train_mrr, label = 'train'); plt.plot(val_mrr, label = 'valid');  plt.legend(loc='upper left')
fig3.suptitle('MRR Vs. Epoch'); plt.xlabel('Epoch'); plt.ylabel('MRR')
plt.axhline(y=nn01_mrr, color='green', linestyle='--')
plt.text(x=0, y=nn01_mrr+0.05, s='NN01 ='+ str(nn01_mrr), fontsize=12, color='green')

#fig3.savefig('diagrams/plots/nn01_mrr.jpg')
#fig3.savefig('diagrams/plots/nn01_s1_mrr.jpg')
#fig3.savefig('diagrams/plots/nn01_s2_mrr.jpg')
""

print(max(train_eval)); print(max(val_eval))

#%%--------------------
""" Plot the graphs """
import matplotlib.pyplot as plt 
import seaborn as sns

epochs = 400
loss_interval, eval_interval, emb_interval = 10, 50, 100 #5, 30, 100

# Plot the loss values

fig1 = plt.figure(1); plt.grid()
plt.plot(range(1,epochs+1), loss_values)
plt.xticks([1] + list(range(100, epochs+1, 100)))
fig1.suptitle('Loss value Vs. Epoch'); plt.xlabel('Epoch'); plt.ylabel('Loss value')
plt.show()

#fig1.savefig('diagrams/two_lossvalues.jpg')

# Plot the hit-rate and MRR
train_hr = [i for i,j in train_eval]; train_mrr = [j for i,j in train_eval] 
val_hr = [i for i,j in val_eval]; val_mrr = [j for i,j in val_eval]  

fig2 = plt.figure(2); plt.grid()
plt.xticks( list(range(len(train_hr))), [eval_interval] + list(range(eval_interval*2, eval_interval * (1+len(train_hr)), eval_interval)))
plt.plot(train_hr, label = 'train'); plt.plot(val_hr, label = 'valid'); plt.legend(loc='center right')
fig2.suptitle('Hit-rate Vs. Epoch'); plt.xlabel('Epoch'); plt.ylabel('Hitrate')

fig3 = plt.figure(3); plt.grid()
plt.xticks( list(range(len(train_mrr))), [eval_interval] + list(range(eval_interval*2, eval_interval * (1+len(train_mrr)), eval_interval)))
plt.plot(train_mrr, label = 'train'); plt.plot(val_mrr, label = 'valid');  plt.legend(loc='center right')
fig3.suptitle('MRR Vs. Epoch'); plt.xlabel('Epoch'); plt.ylabel('MRR')

#fig2.savefig('diagrams/two_hr.jpg')
#fig3.savefig('diagrams/two_mrr.jpg')


#%%---------------------------------
""" 01. Embedding similarity distribution """
import time
import random
import torch
#from scipy.stats import kurtosis 
import torch.nn.functional as F

def embplot(emb, N=2000):
  t0 = time.time()
  ind = random.sample(range(emb.shape[0]), N)
  iemb = emb[ind]
  s=[]; limit = len(iemb)-1
  
  for i,a in enumerate(iemb):
      if i == limit: break
      val = F.cosine_similarity(a[None,:], iemb[i+1:])
      s.extend(val)
  print('Time taken:', time.time()-t0)
  s = torch.stack(s)
  return s.tolist()

#%%---------------------------------
# Plot the embedding similarity distribution curves
from scipy.stats import kurtosis 

fig4 = plt.figure(4); plt.grid()
k = [] #kurtosis_values
for i,emb in enumerate(embs[:4]):
  e = embplot(emb, 2000); k.append(kurtosis(e)); print('Kurtosis number', i, '=',  k[i])
  sns.distplot(e, hist=False, kde = True, norm_hist=True, label = 'Epoch %d' %(emb_interval*(i+1)))

plt.xlim(-1.0,1.0); plt.legend(loc='upper center')
fig4.suptitle('Embedding similarity distribution'); plt.xlabel('Embedding cosine similarity'); plt.ylabel('Probability density of pairwise distances')

#fig4.savefig('diagrams/two_embdist.jpg')
# Kurtosis = [-1.44, -1.45, -1.51, -1.49]

#%%---------------------------------
""" 01. Embedding similarity distribution """
import time
import random
import torch
from scipy.stats import kurtosis 
import torch.nn.functional as F

def embplot(emb):
  t0 = time.time()
  ind = random.sample(range(emb.shape[0]), 2000)
  iemb = emb[ind]
  s=[]; limit = len(iemb)-1
  
  for i,a in enumerate(iemb):
      if i == limit: break
      val = F.cosine_similarity(a[None,:], iemb[i+1:])
      s.extend(val)
  print('Time taken:', time.time()-t0)
  s = torch.stack(s)
  return s.tolist()

#%%---------------------------------
import matplotlib.pyplot as plt
import seaborn as sns

fig1 = plt.figure(1); sns.set_style('whitegrid')

e = embplot(X[:,:3]); k = kurtosis(e); print('01 =', k)
sns.distplot(e, hist=False, kde = True, norm_hist=True, label = 'Numerical')

e = embplot(X[:,3:48]); k = kurtosis(e); print('02 =', k)
sns.distplot(e, hist=False, kde = True, norm_hist=True, label = 'Categorical')

e = embplot(X[:,48:]); k = kurtosis(e); print('03 =', k)
sns.distplot(e, hist=False, kde = True, norm_hist=True, label = 'S-BERT')

e = embplot(X); k = kurtosis(e); print('04 =', k)
sns.distplot(e, hist=False, kde = True, norm_hist=True, label = 'All')

plt.legend(loc='upper left'); plt.xlim(-0.3,1.0)
fig1.suptitle('Embedding similarity distribution'); 
plt.xlabel('Embedding cosine similarity'); plt.ylabel('Probability density of pairwise distances')

#fig1.savefig('diagrams/emb_dist.jpg')

#%%---------------------------------
""" Recommendations on raw vectors """
import time
import pipe
import importlib; importlib.reload(pipe)

t0 = time.time()
onepipe = pipe.pipeflow(X, K=500)
res_train = onepipe.dfmanip(pos)
res_val = onepipe.dfmanip(valpos)
print('Time taken:', time.time()-t0)



!pip install git+https://github.com/pixelogik/NearPy.git#egg=nearpy
#import importlib; importlib.reload(nearpy)

import numpy
import nearpy

#from nearpy import Engine
#from nearpy.hashes import RandomBinaryProjections

# Dimension of our vector space
dimension = 48

# Create a random binary hash with 10 bits
rbp = nearpy.hashes.RandomBinaryProjections('rbp', 10)

# Create engine with pipeline configuration
engine = nearpy.Engine(dimension, lshashes=[rbp])

# Index 1000000 random vectors (set their data to a unique string)
engine.store_many_vectors(X.numpy())
"""
for index in range(100000):
    v = numpy.random.randn(dimension)
    engine.store_vector(v, 'data_%d' % index)
"""
# Create random query vector
#query = X[2].numpy() #numpy.random.randn(dimension)

# Get nearest neighbours
#N = engine.neighbours(query)

# Get nearest neighbours
query = X[2].numpy() 
N = engine.neighbours(query)

#print(query)
print(N[1][0])

import torch
X[2]/torch.norm(X[2])


#%%---------------------------------
"""a. Area under ROC """
def auroc(true, pred):
    from sklearn.metrics import roc_auc_score
    res = (true, pred)
    print("AUROC has been computed and the value is ", res)
    return res