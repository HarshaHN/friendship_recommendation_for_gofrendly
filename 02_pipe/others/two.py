# -*- coding: utf-8 -*-
"""two.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bZbq0ZY1C3gIyePJB9F0N6eGW93PYBG3
"""

import os
os.chdir('/content/drive/My Drive/gofrendly')
#os.getcwd()

!pip install dgl
import torch

device = torch.device('cuda'); #print(device) #cuda
#print(torch.cuda.current_device()) #0
#print(torch.cuda.device(0)) #<torch.cuda.device at 0x7fc1f955b198>
#print(torch.cuda.device_count()) #1
print(torch.cuda.get_device_name(0)) #'Tesla P100'
#print(torch.cuda.is_available()) #True
#print(os.cpu_count()) #2
#!nvidia-smi

import pickle
import dgl
import time

"""
VERSION = "nightly" 
#param ["20200220","nightly", "xrt==1.15.0"]
!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
!python pytorch-xla-env-setup.py --version $VERSION

# imports the torch_xla package
import torch_xla
import torch_xla.core.xla_model as xm

device = xm.xla_device() 
"""

#%%-------------------------------------------------
# Load the files
import pickle
import dgl
 

# 14,948 | 816
with open('data/X.pkl', 'rb') as f: X = pickle.load(f)
with open('data/nw.pkl', 'rb') as f: [pos, neg] = pickle.load(f)
with open('data/valpos.pkl', 'rb') as f: valpos = pickle.load(f)
with open('data/testpos.pkl', 'rb') as f: testpos = pickle.load(f)

 
#with open('one.pkl', 'rb') as f: [emb] = pickle.load(f)
 
"""Move all the tensors to GPU """
import torch
X = X.to(device); print(X.is_cuda); print(X.shape)
#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'); print(device)
 
# pos, neg, valpos = torch.tensor(pos), torch.tensor(neg), torch.tensor(valpos)
# pos, neg, valpos = pos.to(device), neg.to(device), valpos.to(device)
# print(pos.is_cuda, neg.is_cuda, valpos.is_cuda)

import dgl
import time

t0 = time.time()
def makedgl(num, pos):
    G = dgl.DGLGraph()
    G.add_nodes(num)
    G.add_edges(G.nodes(), G.nodes()) #self loop all
    G.add_edges(*zip(*pos)) #add edges list(zip(*pos))
    G = dgl.to_bidirected(G) 
    G = dgl.graph(G.edges(), 'user', 'frd')
    print('-> Graph G has %d nodes' % G.number_of_nodes(), 'with %d edges' % (G.number_of_edges()/2)) 
    return G

G = makedgl(num=len(X), pos=pos)

# Random walk
import dgl
import torch
rw = dgl.sampling.RandomWalkNeighborSampler(G = G, 
                                            random_walk_length = 32,
                                            random_walk_restart_prob=0,
                                            num_random_walks = 32,
                                            num_neighbors = 16,
                                            weight_column = 'w')

rwG = rw(torch.LongTensor(G.nodes()))
rwG.edata['w'] = rwG.edata['w'].float(); 

del rw, rwG.edata['_ID'], G 
print('-> Graph G has %d nodes' % rwG.number_of_nodes(), 'with %d edges' % (rwG.number_of_edges())) # rwG.is_readonly
print('Time taken:', time.time() - t0)

# ng.predecessors(1) # ng.edata['w'][ng.edge_ids(*ng.in_edges(1))]
#with open('data/rwG_s2.pkl', 'wb') as f: pickle.dump(rwG, f)

#%%----------------------
""" 01. Graph Neural Network """

import nn
import pipe
import torch
import time
import importlib; importlib.reload(nn); importlib.reload(pipe)

# x = X
# x = X[:, :3]
# x = X[:, 3:48]
# x = X[:, 48:]
# with open('data/one.pkl', 'rb') as f: x = pickle.load(f)
# neg = neg; pos = pos  #402761: (2134,400627) #72382: (16063,56319)
 
fdim = X.shape[1]
#idx = list(range(X.shape[0])); trainpos = pos
with open('data/rwG_s2.pkl', 'rb') as f: rwG = pickle.load(f) # 121109: 180963, 404803
 
twomodel = nn.gnet( graph = rwG,
                    nodeemb = X, #.to(device),
                    convlayers = [[fdim, fdim]],#, [48,45,45], [45,42,42], [42,39,39], [39,36,36]],
                    #convlayers = [[fdim, fdim], [fdim, fdim, fdim], [fdim, fdim, fdim]],
                    output_size = fdim,
                    dropout = 0.00,
                    lr = 3e-4,
                    opt = 'RMSprop', # Rprop, RMSprop, Adamax, AdamW, Adagrad, Adadelta, SGD, Adam
                    select_loss = 'cosine', #'pinsage'
                    loss_margin = 0.25, # 0.25
                    train_pos = pos,
                    train_neg = neg,
                    val_pos = valpos)
 
twomodel.to(device)
 
# nn01_loss, nn01_hr, nn01_mrr = 0.069, 27.1, 0.8 
print(X.shape)

num_params = sum(p.numel() for p in twomodel.parameters() if p.requires_grad)
print(num_params) 
#1: 9745, 727249 
#2: 2672401, 14869969
#3: 9745

""" 03. a. Training """

 #3e-4 #6e-4 
# lr = 3e-4; epochs = 400; loss_interval, eval_interval, emb_interval = 5, 50, 50 # 
#lr = 1e-5; epochs = 600; loss_interval, eval_interval, emb_interval = 5, 50, 50 # e1
lr = 4e-4; epochs = 480; loss_interval, eval_interval, emb_interval = 5, 50, 50 # e2

intervals = [loss_interval, eval_interval, emb_interval]
twomodel.optimizer  = getattr(torch.optim, 'RMSprop')(twomodel.net.parameters(), lr)
[newemb, train_eval, val_eval, loss_values, embs] = twomodel.train(epochs, intervals)

import pickle
#with open('data/two_training.pkl', 'wb') as f: pickle.dump([newemb, train_eval, val_eval, loss_values, embs], f)
#with open('data/two_s1_training.pkl', 'wb') as f: pickle.dump([newemb, train_eval, val_eval, loss_values, embs], f)
#with open('data/two_s2_training.pkl', 'wb') as f: pickle.dump([newemb, train_eval, val_eval, loss_values, embs], f)
#with open('data/two.pkl', 'wb') as f: pickle.dump(embs[-1], f)
#with open('data/two_s1.pkl', 'wb') as f: pickle.dump(embs[3], f)
#with open('data/two_s2.pkl', 'wb') as f: pickle.dump(embs[-3], f)

#with open('data/two_training.pkl', 'rb') as f: [newemb, train_eval, val_eval, loss_values, embs] = pickle.load(f); epochs = 400; loss_interval, eval_interval, emb_interval = 5, 50, 100 #400, 50, 100 #3e-4
#with open('data/two_s1_training.pkl', 'rb') as f: [newemb, train_eval, val_eval, loss_values, embs] = pickle.load(f) #480, 60, 60
#with open('data/two_s2_training.pkl', 'rb') as f: [newemb, train_eval, val_eval, loss_values, embs] = pickle.load(f); epochs = 480; loss_interval, eval_interval, emb_interval = 5, 60, 60 #400, 50, 100 #3e-4
#with open('data/two.pkl', 'rb') as f: two = pickle.load(f)

#nn01_loss, nn01_hr, nn01_mrr = 0.079, 27.1, 0.8 
#nn02_loss, nn02_hr, nn02_mrr = 0.073, 34.1, 1.0
#nn_loss, nn_hr, nn_mrr = 0.073, 34.1, 1.0

""" Test data """
import time
import pipe
import importlib; importlib.reload(pipe)

t0 = time.time()

#with open('data/two.pkl', 'rb') as f: two = pickle.load(f)
#with open('data/two_s1.pkl', 'rb') as f: two = pickle.load(f)
#with open('data/two_s2.pkl', 'rb') as f: two = pickle.load(f)[idx]
#with open('data/testpos.pkl', 'rb') as f: testpos = pickle.load(f)

onepipe = pipe.pipeflow(X[idx], K=500)

res_train = onepipe.dfmanip(trainpos)
res_val = onepipe.dfmanip(valpos)
res_test = onepipe.dfmanip(testpos)

print('Time taken:', time.time()-t0)

# Hitrate = 28.3 MRR = 2.2, Hitrate = 26.2 MRR = 1.0, Hitrate = 24.9 MRR = 1.1 #Non-learned
# Hitrate = 21.8 MRR = 1.6, Hitrate = 19.1 MRR = 0.6, Hitrate = 18.5 MRR = 0.9 #Non-learned E1
# Hitrate = 28.1 MRR = 2.2, Hitrate = 26.7 MRR = 0.9, Hitrate = 25.0 MRR = 1.0 #Non-learned E2
# Hitrate = 52.0 MRR = 4.8, Hitrate = 34.1 MRR = 1.0, Hitrate = 29.6 MRR = 1.2
# Hitrate = 65.6, MRR = 8.6, Hitrate = 34.6 MRR = 1.4, Hitrate = 31.6 MRR = 1.5 #E1
# Hitrate = 34.8 MRR = 2.6, Hitrate = 29.2 MRR = 0.9, Hitrate = 25.7 MRR = 1.0 #E2

import matplotlib.pyplot as plt 
import seaborn as sns

""
# Plot the loss values
fig1 = plt.figure(1); plt.grid()
plt.plot(range(1,epochs+1), loss_values)
plt.xticks([1] + list(range(100, epochs+1, 100)))
fig1.suptitle('Loss value Vs. Epoch'); plt.xlabel('Epoch'); plt.ylabel('Loss value')
#plt.axhline(y=nn_loss, color='green', linestyle='--')
#plt.text(x=0, y=nn_loss+0.02, s='NN02 ='+str(nn_loss), fontsize=12, color='green')
plt.show()

""

# Plot the hit-rate and MRR
train_hr = [i for i,j in train_eval]; train_mrr = [j for i,j in train_eval] 
val_hr = [i for i,j in val_eval]; val_mrr = [j for i,j in val_eval]  

fig2 = plt.figure(2); plt.grid()
plt.xticks( list(range(len(train_hr))), [eval_interval] + list(range(eval_interval*2, eval_interval * (1+len(train_hr)), eval_interval)))
plt.plot(train_hr, label = 'train'); plt.plot(val_hr, label = 'valid'); plt.legend(loc='upper left')
fig2.suptitle('Hit-rate Vs. Epoch'); plt.xlabel('Epoch'); plt.ylabel('Hitrate')
# plt.plot([nn02_hr]*len(val_hr), label = 'NN02');
#plt.axhline(y=nn_hr, color='green', linestyle='--')
#plt.text(x=3, y=nn_hr+0.5, s='NN02 =' + str(nn_hr), fontsize=12, color='green')


""
fig3 = plt.figure(3); plt.grid()
plt.xticks( list(range(len(train_mrr))), [eval_interval] + list(range(eval_interval*2, eval_interval * (1+len(train_mrr)), eval_interval)))
plt.plot(train_mrr, label = 'train'); plt.plot(val_mrr, label = 'valid');  plt.legend(loc='upper left')
fig3.suptitle('MRR Vs. Epoch'); plt.xlabel('Epoch'); plt.ylabel('MRR')
#plt.axhline(y=nn_mrr, color='green', linestyle='--')
#plt.text(x=0, y=nn_mrr+0.05, s='NN02 ='+ str(nn_mrr), fontsize=12, color='green')
""

""
#fig1.savefig('diagrams/plots/gcn/lossvalues.jpg')
#fig1.savefig('diagrams/plots/gcn/s1_lossvalues.jpg')
fig1.savefig('diagrams/plots/gcn/s2_lossvalues.jpg')

#fig2.savefig('diagrams/plots/gcn/hr.jpg')
#fig2.savefig('diagrams/plots/gcn/s1_hr.jpg')
fig2.savefig('diagrams/plots/gcn/s2_hr.jpg')

#fig3.savefig('diagrams/plots/gcn/mrr.jpg')
#fig3.savefig('diagrams/plots/gcn/s1_mrr.jpg')
fig3.savefig('diagrams/plots/gcn/s2_mrr.jpg')
""

print(max(train_eval)); print(max(val_eval))

#%%--------------------
""" Plot the graphs """
import matplotlib.pyplot as plt 
import seaborn as sns

epochs = 400
loss_interval, eval_interval, emb_interval = 10, 50, 100 #5, 30, 100

# Plot the loss values

fig1 = plt.figure(1); plt.grid()
plt.plot(range(1,epochs+1), loss_values)
plt.xticks([1] + list(range(100, epochs+1, 100)))
fig1.suptitle('Loss value Vs. Epoch'); plt.xlabel('Epoch'); plt.ylabel('Loss value')
plt.show()

#fig1.savefig('diagrams/two_lossvalues.jpg')

# Plot the hit-rate and MRR
train_hr = [i for i,j in train_eval]; train_mrr = [j for i,j in train_eval] 
val_hr = [i for i,j in val_eval]; val_mrr = [j for i,j in val_eval]  

fig2 = plt.figure(2); plt.grid()
plt.xticks( list(range(len(train_hr))), [eval_interval] + list(range(eval_interval*2, eval_interval * (1+len(train_hr)), eval_interval)))
plt.plot(train_hr, label = 'train'); plt.plot(val_hr, label = 'valid'); plt.legend(loc='center right')
fig2.suptitle('Hit-rate Vs. Epoch'); plt.xlabel('Epoch'); plt.ylabel('Hitrate')

fig3 = plt.figure(3); plt.grid()
plt.xticks( list(range(len(train_mrr))), [eval_interval] + list(range(eval_interval*2, eval_interval * (1+len(train_mrr)), eval_interval)))
plt.plot(train_mrr, label = 'train'); plt.plot(val_mrr, label = 'valid');  plt.legend(loc='center right')
fig3.suptitle('MRR Vs. Epoch'); plt.xlabel('Epoch'); plt.ylabel('MRR')

#fig2.savefig('diagrams/two_hr.jpg')
#fig3.savefig('diagrams/two_mrr.jpg')



""" 01. Embedding similarity distribution """
import time
import random
import torch
#from scipy.stats import kurtosis 
import torch.nn.functional as F

def embplot(emb, N=2000):
  t0 = time.time()
  ind = random.sample(range(emb.shape[0]), N)
  iemb = emb[ind]
  s=[]; limit = len(iemb)-1
  
  for i,a in enumerate(iemb):
      if i == limit: break
      val = F.cosine_similarity(a[None,:], iemb[i+1:])
      s.extend(val)
  print('Time taken:', time.time()-t0)
  s = torch.stack(s)
  return s.tolist()

# Plot the embedding similarity distribution curves
from scipy.stats import kurtosis 

fig4 = plt.figure(4); plt.grid()
k = [] #kurtosis_values
for i,emb in enumerate(embs[:4]):
  e = embplot(emb, 2000); k.append(kurtosis(e)); print('Kurtosis number', i, '=',  k[i])
  sns.distplot(e, hist=False, kde = True, norm_hist=True, label = 'Epoch %d' %(emb_interval*(i+1)))

plt.xlim(-1.0,1.0); plt.legend(loc='upper center')
fig4.suptitle('Embedding similarity distribution'); plt.xlabel('Embedding cosine similarity'); plt.ylabel('Probability density of pairwise distances')

#fig4.savefig('diagrams/two_embdist.jpg')
# Kurtosis = [-1.44, -1.45, -1.51, -1.49]

""" 01. Embedding similarity distribution """
import time
import random
import torch
from scipy.stats import kurtosis 
import torch.nn.functional as F

def embplot(emb):
  t0 = time.time()
  ind = random.sample(range(emb.shape[0]), 2000)
  iemb = emb[ind]
  s=[]; limit = len(iemb)-1
  
  for i,a in enumerate(iemb):
      if i == limit: break
      val = F.cosine_similarity(a[None,:], iemb[i+1:])
      s.extend(val)
  print('Time taken:', time.time()-t0)
  s = torch.stack(s)
  return s.tolist()

import matplotlib.pyplot as plt
import seaborn as sns

fig1 = plt.figure(1); sns.set_style('whitegrid')

e = embplot(X[:,:3]); k = kurtosis(e); print('01 =', k)
sns.distplot(e, hist=False, kde = True, norm_hist=True, label = 'Numerical')

e = embplot(X[:,3:48]); k = kurtosis(e); print('02 =', k)
sns.distplot(e, hist=False, kde = True, norm_hist=True, label = 'Categorical')

e = embplot(X[:,48:]); k = kurtosis(e); print('03 =', k)
sns.distplot(e, hist=False, kde = True, norm_hist=True, label = 'S-BERT')

e = embplot(X); k = kurtosis(e); print('04 =', k)
sns.distplot(e, hist=False, kde = True, norm_hist=True, label = 'All')

plt.legend(loc='upper left'); plt.xlim(-0.3,1.0)
fig1.suptitle('Embedding similarity distribution'); 
plt.xlabel('Embedding cosine similarity'); plt.ylabel('Probability density of pairwise distances')

#fig1.savefig('diagrams/emb_dist.jpg')

""" Recommendations on raw vectors """
import time
import pipe
import importlib; importlib.reload(pipe)

t0 = time.time()

with open('data/two.pkl', 'rb') as f: two = pickle.load(f)
with open('data/testpos.pkl', 'rb') as f: testpos = pickle.load(f)

onepipe = pipe.pipeflow(two, K=500)

res_train = onepipe.dfmanip(pos)
#res_val = onepipe.dfmanip(valpos)
res_test = onepipe.dfmanip(testpos)

print('Time taken:', time.time()-t0)

# Hitrate = 52.0 MRR = 4.8, Hitrate = 34.1 MRR = 1.0, Hitrate = 29.6 MRR = 1.2
#



!pip install git+https://github.com/pixelogik/NearPy.git#egg=nearpy
#import importlib; importlib.reload(nearpy)

import numpy
import nearpy

#from nearpy import Engine
#from nearpy.hashes import RandomBinaryProjections

# Dimension of our vector space
dimension = 48

# Create a random binary hash with 10 bits
rbp = nearpy.hashes.RandomBinaryProjections('rbp', 10)

# Create engine with pipeline configuration
engine = nearpy.Engine(dimension, lshashes=[rbp])

# Index 1000000 random vectors (set their data to a unique string)
engine.store_many_vectors(X.numpy())
"""
for index in range(100000):
    v = numpy.random.randn(dimension)
    engine.store_vector(v, 'data_%d' % index)
"""
# Create random query vector
#query = X[2].numpy() #numpy.random.randn(dimension)

# Get nearest neighbours
#N = engine.neighbours(query)

# Get nearest neighbours
query = X[2].numpy() 
N = engine.neighbours(query)

#print(query)
print(N[1][0])

import torch
X[2]/torch.norm(X[2])

"""a. Area under ROC """
def auroc(true, pred):
    from sklearn.metrics import roc_auc_score
    res = (true, pred)
    print("AUROC has been computed and the value is ", res)
    return res

with open('data/short_X.pkl', 'rb') as f: x = pickle.load(f)[1].to(device)
torch.all(torch.eq(X[idx], x))
#torch.eq(X, x)
count = 0
for i in range(len(x)):
  e = x[i] == X[i]
  if not e[0]:
  #if not torch.all(e):
    count += 1
    #print(i, e[0])
    #if count >= 100: break
print(count)